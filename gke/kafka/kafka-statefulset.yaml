# GKE-Optimized Kafka StatefulSet with KRaft mode

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: iot-pipeline
  labels:
    app: kafka
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: kafka
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9101"
    spec:
      # Set security context for the pod: allow group write for mounted volume
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsNonRoot: true

      # Anti-affinity to spread pods across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - kafka
              topologyKey: kubernetes.io/hostname
      
      # Init container for storage formatting
      initContainers:
      - name: kafka-init
        image: confluentinc/cp-kafka:7.9.0
        command:
        - sh
        - -exc
        - |
          NODE_ID=${POD_NAME##*-}
          echo "Initializing Kafka storage for node ${NODE_ID}"
          
          # Remove lost+found directory if it exists
          if [ -d /var/lib/kafka/data/lost+found ]; then
            echo "Removing lost+found directory..."
            rm -rf /var/lib/kafka/data/lost+found
          fi

          if [ -f /var/lib/kafka/data/meta.properties ]; then
            echo "Storage already formatted, skipping format step"
          else
            echo "Formatting storage for KRaft mode..."
            cat > /tmp/server.properties <<EOF
          process.roles=broker,controller
          node.id=${NODE_ID}
          controller.quorum.voters=0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093
          listeners=PLAINTEXT://:9092,CONTROLLER://:9093
          advertised.listeners=PLAINTEXT://${POD_NAME}.kafka-headless.iot-pipeline.svc.cluster.local:9092
          controller.listener.names=CONTROLLER
          log.dirs=/var/lib/kafka/data
          EOF
            
            kafka-storage format \
              --cluster-id="${CLUSTER_ID}" \
              --config=/tmp/server.properties \
              --ignore-formatted
            
            echo "Storage formatting complete"
          fi
        env:
        - name: CLUSTER_ID
          valueFrom:
            secretKeyRef:
              name: iot-pipeline-secrets
              key: CLUSTER_ID
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      
      # Main Kafka container
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.9.0
        ports:
        - containerPort: 9092
          name: kafka
        - containerPort: 9093
          name: controller
        - containerPort: 7071
          name: jmx
        command:
        - sh
        - -exc
        - |
          # Unset deprecated PORT variable
          unset PORT
          unset KAFKA_PORT

          # Export cluster and node identifiers
          export NODE_ID=${POD_NAME##*-}
          export KAFKA_NODE_ID=${NODE_ID}
          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.iot-pipeline.svc.cluster.local:9092
          export KAFKA_CONTROLLER_QUORUM_VOTERS="0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093"
          
          # Verify storage is formatted
          if [ ! -f /var/lib/kafka/data/meta.properties ]; then
            echo "ERROR: Storage not formatted! Init container may have failed."
            exit 1
          fi

          echo "Starting Kafka node ${NODE_ID} in KRaft mode..."
          echo "Advertised Listeners: ${KAFKA_ADVERTISED_LISTENERS}"
          echo "Controller Quorum: ${KAFKA_CONTROLLER_QUORUM_VOTERS}"
          
          # Start Kafka using Confluent's entrypoint
          exec /etc/confluent/docker/run
        env:
        - name: CLUSTER_ID
          valueFrom:
            secretKeyRef:
              name: iot-pipeline-secrets
              key: CLUSTER_ID
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        
        # CORE KAFKA SETTINGS
        - name: KAFKA_PROCESS_ROLES
          value: "broker,controller"
        - name: KAFKA_CONTROLLER_LISTENER_NAMES
          value: "CONTROLLER"
        - name: KAFKA_LISTENERS
          value: PLAINTEXT://:9092,CONTROLLER://:9093
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "PLAINTEXT"

        # REPLICATION SETTINGS
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
          value: "2"
        - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_DEFAULT_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_MIN_INSYNC_REPLICAS
          value: "2"

        # TIMEOUT SETTINGS
        - name: KAFKA_REPLICA_SOCKET_TIMEOUT_MS
          value: "60000"  # ⬅️ Increased from default 30s to 60s
        - name: KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS
          value: "60000"  # ⬅️ Controller timeout (critical!)
        - name: KAFKA_REPLICA_LAG_TIME_MAX_MS
          value: "30000"  # ⬅️ How long before replica considered out of sync
        - name: KAFKA_REQUEST_TIMEOUT_MS
          value: "60000"  # ⬅️ Client request timeout
        - name: KAFKA_CONNECTIONS_MAX_IDLE_MS
          value: "600000"  # ⬅️ Keep connections alive for 10 minutes

        # NETWORK SETTINGS (IMPROVE RELIABILITY)
        - name: KAFKA_SOCKET_SEND_BUFFER_BYTES
          value: "1048576"  # ⬅️ 1MB send buffer (increased from 100KB default)
        - name: KAFKA_SOCKET_RECEIVE_BUFFER_BYTES
          value: "1048576"  # ⬅️ 1MB receive buffer
        - name: KAFKA_SOCKET_REQUEST_MAX_BYTES
          value: "104857600"  # ⬅️ 100MB max request size
        - name: KAFKA_REPLICA_FETCH_MAX_BYTES
          value: "10485760"  # ⬅️ 10MB max fetch size
        - name: KAFKA_REPLICA_FETCH_WAIT_MAX_MS
          value: "1000"  # ⬅️ Replica fetch wait time

        # LOG SETTINGS (REDUCE I/O SPIKES)
        - name: KAFKA_LOG_SEGMENT_BYTES
          value: "1073741824"  # ⬅️ 1GB segments (prevents frequent rolling)
        - name: KAFKA_LOG_ROLL_MS
          value: "604800000"  # ⬅️ Roll logs weekly
        - name: KAFKA_LOG_FLUSH_INTERVAL_MESSAGES
          value: "10000"  # ⬅️ Flush every 10k messages
        - name: KAFKA_LOG_FLUSH_INTERVAL_MS
          value: "1000"  # ⬅️ Or flush every second

        # GENERAL SETTINGS
        - name: KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS
          value: "3000"
        - name: KAFKA_NUM_PARTITIONS
          value: "6"
        - name: KAFKA_LOG_RETENTION_MS
          value: "604800000"
        - name: KAFKA_LOG_CLEANUP_POLICY
          value: "delete"
        - name: KAFKA_LOG_DIRS
          value: "/var/lib/kafka/data"
        - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
          value: "true"

        # JVM SETTINGS          
        - name: KAFKA_JMX_PORT
          value: "7071"
        - name: KAFKA_HEAP_OPTS
          value: "-Xmx1G -Xms1G" # Increased from 16 to 26
        

        # Disable Java DNS caching
        - name: KAFKA_OPTS
          value: >-
            -Dnetworkaddress.cache.ttl=0
            -Dnetworkaddress.cache.negative.ttl=0 -XX:+UseG1GC
            -XX:MaxGCPauseMillis=20
            -XX:InitiatingHeapOccupancyPercent=35
            -XX:G1HeapRegionSize=16M
            -XX:MinMetaspaceFreeRatio=50
            -XX:MaxMetaspaceFreeRatio=80
        
        # Explicitly unset PORT to avoid conflicts
        - name: PORT
          value: ""
        
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - ps aux | grep -q "[k]afka.Kafka" || exit 1
          initialDelaySeconds: 90
          periodSeconds: 15
          timeoutSeconds: 10
          failureThreshold: 6
        
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - nc -z localhost 9092 || exit 1
          initialDelaySeconds: 60
          periodSeconds: 15
          timeoutSeconds: 15
          failureThreshold: 6
      
      # JMX Exporter sidecar
      - name: jmx-exporter
        image: bitnami/jmx-exporter:latest
        ports:
        - containerPort: 9101
          name: metrics
        args:
        - "9101"
        - /etc/jmx-exporter/config.yml
        
        volumeMounts:
        - name: jmx-config
          mountPath: /etc/jmx-exporter/config.yml
          subPath: kafka-jmx-config.yml
        
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      
      volumes:
      - name: jmx-config
        configMap:
          name: kafka-jmx-config
  
  volumeClaimTemplates:
  - metadata:
      name: kafka-data
      labels:
        app: kafka
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: premium-rwo
      resources:
        requests:
          storage: 50Gi


