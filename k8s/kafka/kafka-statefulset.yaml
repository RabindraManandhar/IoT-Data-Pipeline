# Kafka StatefulSet with KRaft mode (no Zookeeper required)

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: iot-pipeline
  labels:
    app: kafka
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  podManagementPolicy: Parallel  # Start all pods in parallel
  template:
    metadata:
      labels:
        app: kafka
    spec:
      # This runs ONCE before the main Kafka container starts
      initContainers:      
      # Formats Kafka storage for KRaft mode
      - name: kafka-init
        image: confluentinc/cp-kafka:7.9.0
        command:
        - sh
        - -exc
        - |
          # Extract node ID from pod name (kafka-0 -> 0, kafka-1 -> 1, kafka-2 -> 2)
          NODE_ID=${POD_NAME##*-}
          
          echo "Initializing Kafka storage for node ${NODE_ID}"
          
          # Check if storage is already formatted (meta.properties file indicates formatted storage)
          if [ -f /var/lib/kafka/data/meta.properties ]; then
            echo "Storage already formatted, skipping format step"
          else
            echo "Formatting storage for KRaft mode..."
            
            # Create a temporary server.properties for formatting
            cat > /tmp/server.properties <<EOF
          process.roles=broker,controller
          node.id=${NODE_ID}
          controller.quorum.voters=0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093
          listeners=PLAINTEXT://:9092,CONTROLLER://:9093
          advertised.listeners=PLAINTEXT://${POD_NAME}.kafka-headless.iot-pipeline.svc.cluster.local:9092
          controller.listener.names=CONTROLLER
          log.dirs=/var/lib/kafka/data
          EOF
            
            # Format the storage with the cluster ID
            kafka-storage format \
              --cluster-id="${CLUSTER_ID}" \
              --config=/tmp/server.properties \
              --ignore-formatted
            
            echo "Storage formatting complete"
          fi
          
          echo "Init container completed successfully"
        env:
        - name: CLUSTER_ID
          valueFrom:
            secretKeyRef:
              name: iot-pipeline-secrets
              key: CLUSTER_ID
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      
      # Main Kafka container
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.9.0
        ports:
        - containerPort: 9092
          name: kafka
          protocol: TCP
        - containerPort: 9093
          name: controller
          protocol: TCP
        - containerPort: 7071
          name: jmx
          protocol: TCP
        command:
        - sh
        - -exc
        - |
          # Export cluster and node identifiers
          export NODE_ID=${POD_NAME##*-}
          export KAFKA_NODE_ID=${NODE_ID}
          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.iot-pipeline.svc.cluster.local:9092
          export KAFKA_CONTROLLER_QUORUM_VOTERS="0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093"
          
          # Verify storage is formatted
          if [ ! -f /var/lib/kafka/data/meta.properties ]; then
            echo "ERROR: Storage not formatted! Init container may have failed."
            exit 1
          fi
          
          # Start Kafka using Confluent's entrypoint
          exec /etc/confluent/docker/run
        
        env:
        # Cluster ID (from secret)
        - name: CLUSTER_ID
          valueFrom:
            secretKeyRef:
              name: iot-pipeline-secrets
              key: CLUSTER_ID

        # Hostname for pod identity
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        
        # ===== KRaft Mode Configuration =====
        
        # Process roles: this node is both broker and controller
        - name: KAFKA_PROCESS_ROLES
          value: "broker,controller"
        
        # Controller quorum voters: exported in startup command
        - name: KAFKA_CONTROLLER_LISTENER_NAMES
          value: "CONTROLLER"

        - name: KAFKA_LISTENERS
          value: PLAINTEXT://:9092,CONTROLLER://:9093
        
        # Security protocol map
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
        
        # Inter-broker listener
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "PLAINTEXT"
        
        # ===== Replication & Durability =====
        
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "3"
        
        - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
          value: "2"

        - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
          value: "3"
        
        - name: KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS
          value: "3000"

        - name: KAFKA_NUM_PARTITIONS
          value: "6"
       
        - name: KAFKA_LOG_RETENTION_MS
          value: "604800000"
       
        - name: KAFKA_LOG_CLEANUP_POLICY
          value: "delete"
        
        # ===== Storage Configuration =====
        - name: KAFKA_LOG_DIRS
          value: "/var/lib/kafka/data"
        
        # ===== JMX Monitoring =====
        - name: KAFKA_JMX_PORT
          value: "7071"

        # ==== Java Heap Settings ====
        - name: KAFKA_HEAP_OPTS
          value: "-Xmx512M -Xms512M"

        # ==== Other Settings ====
        - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
          value: "true"

        - name: KAFKA_PORT
          value: ""

        # Disable Java DNS caching
        - name: KAFKA_OPTS
          value: "-Dnetworkaddress.cache.ttl=0 -Dnetworkaddress.cache.negative.ttl=0"
        
        # Volume mounts
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        
        # Resource limits
        resources:
          requests:
            memory: "512Mi"
            cpu: "300m"
          limits:
            memory: "1Gi"
            cpu: "800m"
        
        # ===== Health checks with appropriate timeouts =====
        livenessProbe:
          exec:
            command:
              - sh
              - -c
              - >
                ps aux | grep -q "[k]afka.Kafka" || exit 1
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 5
        
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - >
                nc -z localhost 9092 || exit 1
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 5
      
      # Add JMX Exporter as sidecar
      - name: jmx-exporter
        image: bitnami/jmx-exporter:latest
        ports:
        - containerPort: 9101
          name: metrics
        args:
        - "9101"
        - /etc/jmx-exporter/config.yml
        volumeMounts:
        - name: jmx-config
          mountPath: /etc/jmx-exporter/config.yml
          subPath: kafka-jmx-config.yml
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
      
      volumes:
      - name: jmx-config
        configMap:
          name: kafka-jmx-config

  # Persistent volume claim templates
  volumeClaimTemplates:
  - metadata:
      name: kafka-data
      labels:
        app: kafka
    spec:
      accessModes: 
      - ReadWriteOnce
      storageClassName: standard
      resources:
        requests:
          storage: 10Gi