# Kafka StatefulSet with KRaft mode (no Zookeeper required)

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: iot-pipeline
  labels:
    app: kafka
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  podManagementPolicy: Parallel  # Start all pods consecutively
  template:
    metadata:
      labels:
        app: kafka
    spec:
      # This runs ONCE before the main Kafka container starts
      initContainers:
      # DNS check init container
      - name: dns-check
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          echo "Checking DNS resolution for Kafka pods..."
          TIMEOUT=300
          START_TIME=$(date +%s)
          for i in 0 1 2; do
            HOST="kafka-${i}.kafka-headless.iot-pipeline.svc.cluster.local"
            echo "Resolving $HOST..."
            while true; do
              if nslookup $HOST; then
                echo "$HOST resolved successfully"
                break
              fi
              CURRENT_TIME=$(date +%s)
              ELAPSED=$((CURRENT_TIME - START_TIME))
              if [ $ELAPSED -ge $TIMEOUT ]; then
                echo "ERROR: Timeout after ${TIMEOUT}s resolving $HOST"
                exit 1
              fi
              echo "Failed to resolve $HOST, retrying in 5 seconds..."
              sleep 5
            done
          done
          echo "All Kafka DNS names resolved"
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"

      # Formats Kafka storage for KRaft mode
      - name: kafka-init
        image: confluentinc/cp-kafka:7.9.0
        command:
        - sh
        - -exc
        - |
          # Extract node ID from pod name (kafka-0 -> 0, kafka-1 -> 1, kafka-2 -> 2)
          NODE_ID=${POD_NAME##*-}
          
          echo "Initializing Kafka storage for node ${NODE_ID}"
          
          # Check if storage is already formatted (meta.properties file indicates formatted storage)
          if [ -f /var/lib/kafka/data/meta.properties ]; then
            echo "Storage already formatted, skipping format step"
          else
            echo "Formatting storage for KRaft mode..."
            
            # Create a temporary server.properties for formatting
            cat > /tmp/server.properties <<EOF
          process.roles=broker,controller
          node.id=${NODE_ID}
          controller.quorum.voters=0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093
          listeners=PLAINTEXT://:9092,CONTROLLER://:9093
          advertised.listeners=PLAINTEXT://${POD_NAME}.kafka-headless.iot-pipeline.svc.cluster.local:9092
          controller.listener.names=CONTROLLER
          log.dirs=/var/lib/kafka/data
          EOF
            
            # Format the storage with the cluster ID
            kafka-storage format \
              --cluster-id="${CLUSTER_ID}" \
              --config=/tmp/server.properties \
              --ignore-formatted
            
            echo "Storage formatting complete"
          fi
          
          echo "Init container completed successfully"
        env:
        - name: CLUSTER_ID
          valueFrom:
            secretKeyRef:
              name: iot-pipeline-secrets
              key: CLUSTER_ID
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      
      # Main Kafka container
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.9.0
        ports:
        - containerPort: 9092
          name: kafka
          protocol: TCP
        - containerPort: 9093
          name: controller
          protocol: TCP
        - containerPort: 7071
          name: jmx
          protocol: TCP
        command:
        - sh
        - -exc
        - |
          # Export cluster and node identifiers
          export NODE_ID=${POD_NAME##*-}
          export KAFKA_NODE_ID=${NODE_ID}
          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.iot-pipeline.svc.cluster.local:9092
          export KAFKA_CONTROLLER_QUORUM_VOTERS="0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093"
          
          # Verify storage is formatted
          if [ ! -f /var/lib/kafka/data/meta.properties ]; then
            echo "ERROR: Storage not formatted! Init container may have failed."
            exit 1
          fi
          
          # Start Kafka using Confluent's entrypoint
          exec /etc/confluent/docker/run
        
        env:
        # Cluster ID (from secret)
        - name: CLUSTER_ID
          valueFrom:
            secretKeyRef:
              name: iot-pipeline-secrets
              key: CLUSTER_ID

        # Hostname for pod identity
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        
        # ===== KRaft Mode Configuration =====
        
        # Process roles: this node is both broker and controller
        - name: KAFKA_PROCESS_ROLES
          value: "broker,controller"
        
        # Node ID: extracted from hostname in startup command
        # Format: 0, 1, or 2 based on pod ordinal
        
        # Controller quorum voters: exported in startup command
        # Format: id@host:port,id@host:port,...)

        # ===== Listener Configuration =====
        
        # PLAINTEXT: client connections (port 9092)
        # CONTROLLER: inter-controller communication (port 9093)

        - name: KAFKA_CONTROLLER_LISTENER_NAMES
          value: "CONTROLLER"

        - name: KAFKA_LISTENERS
          value: PLAINTEXT://:9092,CONTROLLER://:9093

        # Advertised listeners: set dynamically in startup command
        # (removed from here to avoid variable expansion issues)
        
        # Security protocol map
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
        
        # Inter-broker listener: used for broker-to-broker communication
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "PLAINTEXT"
        
        # ===== Replication & Durability =====
        
        # Offsets topic replication (internal topic)
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "3"  # Matches kafka.replication_factor: 3
        
        # Transaction state log settings (for exactly-once semantics)
        - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
          value: "2"    # Matches kafka.topic_config.min_insync_replicas: 2

        - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
          value: "3"  # Matches kafka.replication_factor: 3
        
        # Consumer group initial rebalance delay
        - name: KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS
          value: "3000"

        # Default number of partitions for auto-created topics (from configmap.yaml: kafka.partitions)
        - name: KAFKA_NUM_PARTITIONS
          value: "6"  # Matches kafka.partitions: 6
       
        # Log retention (from configmap.yaml: kafka.topic_config.retention_ms)
        - name: KAFKA_LOG_RETENTION_MS
          value: "604800000"  # Matches kafka.topic_config.retention_ms: 604800000 (7 days)
       
        # Cleanup policy (from configmap.yaml: kafka.topic_config.cleanup_policy)
        - name: KAFKA_LOG_CLEANUP_POLICY
          value: "delete"  # Matches kafka.topic_config.cleanup_policy: delete
        
        # ===== Storage Configuration =====

        # Where Kafka stores data (must match init container)
        - name: KAFKA_LOG_DIRS
          value: "/var/lib/kafka/data"
        
        # ===== JMX Monitoring =====
        
        - name: KAFKA_JMX_PORT
          value: "7071"

        # ==== Java Heap Settings - CRITICAL FOR RESOURCE CONSTRAINTS ====
        
        - name: KAFKA_HEAP_OPTS
          value: "-Xmx512M -Xms512M" # Limited heap for low-resource environment

        # ==== Other Settings ====
        
        # Auto-create topics when producer sends data
        - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
          value: "true"

        # Remove deprecated port configuration
        - name: KAFKA_PORT
          value: ""  # Override any default port setting

        # Disable Java DNS caching to prevent cached failures in K8s
        - name: KAFKA_OPTS
          value: "-Dnetworkaddress.cache.ttl=0 -Dnetworkaddress.cache.negative.ttl=0"
        
        # Volume mounts
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        
        # Resource limits
        resources:
          requests:
            memory: "512Mi"
            cpu: "300m"
          limits:
            memory: "1Gi"
            cpu: "800m"
        
        # ===== Health checks with longer timeouts =====

        # Liveness probe: is Kafka running?
        livenessProbe:
          exec:
            command:
              - sh
              - -c
              - >
                ps aux | grep -q "[k]afka.Kafka" || exit 1
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 5
        
        # Readiness probe: is Kafka ready for traffic?
        readinessProbe:
          exec:
            command:
              - sh
              - c
              - >
                nc -z localhost 9092 || exit 1
          initialDelaySeconds: 20
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 5
      
  # Persistent volume claim templates
  volumeClaimTemplates:
  - metadata:
      name: kafka-data
      labels:
        app: kafka
    spec:
      accessModes: 
      - ReadWriteOnce
      storageClassName: standard
      resources:
        requests:
          storage: 10Gi
