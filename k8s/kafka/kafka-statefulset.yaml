# Kafka StatefulSet with KRaft mode (no Zookeeper required)
# KRaft = Kafka Raft metadata mode, introduced in Kafka 2.8+

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: iot-pipeline
  labels:
    app: kafka
    component: broker
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  podManagementPolicy: Parallel  # Start all pods simultaneously
  template:
    metadata:
      labels:
        app: kafka
        component: broker
    spec:
      # Init container: Formats Kafka storage for KRaft mode
      # This runs ONCE before the main Kafka container starts
      initContainers:
      - name: kafka-init
        image: confluentinc/cp-kafka:7.9.0
        command:
        - /bin/bash
        - -c
        - |
          set -ex
          
          # Extract node ID from pod name
          # kafka-0 -> 0, kafka-1 -> 1, kafka-2 -> 2
          NODE_ID=${HOSTNAME##*-}
          echo "===== Kafka Init Container ====="
          echo "Pod Name: ${HOSTNAME}"
          echo "Node ID: ${NODE_ID}"
          echo "Cluster ID: ${CLUSTER_ID}"
          echo "================================"
          
          # Check if storage is already formatted
          # meta.properties file indicates formatted storage
          if [ -f /var/lib/kafka/data/meta.properties ]; then
            echo "Storage already formatted, skipping format step"
            cat /var/lib/kafka/data/meta.properties
          else
            echo "Formatting storage for KRaft mode..."
            
            # Create a temporary server.properties for formatting
            cat > /tmp/server.properties <<EOF
          process.roles=broker,controller
          node.id=${NODE_ID}
          controller.quorum.voters=0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093
          listeners=PLAINTEXT://:9092,CONTROLLER://:9093
          controller.listener.names=CONTROLLER
          log.dirs=/var/lib/kafka/data
          EOF
            
            # Format the storage with the cluster ID
            kafka-storage format \
              --cluster-id="${CLUSTER_ID}" \
              --config=/tmp/server.properties \
              --ignore-formatted
            
            echo "Storage formatting complete"
            cat /var/lib/kafka/data/meta.properties
          fi
          
          echo "Init container completed successfully"
        env:
        - name: CLUSTER_ID
          valueFrom:
            secretKeyRef:
              name: iot-pipeline-secrets
              key: CLUSTER_ID
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      
      # Main Kafka container
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.9.0
        ports:
        - containerPort: 9092
          name: kafka
          protocol: TCP
        - containerPort: 9093
          name: controller
          protocol: TCP
        - containerPort: 7071
          name: jmx
          protocol: TCP
        
        # Startup command
        command:
        - /bin/bash
        - -c
        - |
          set -ex
          
          # Extract node ID from pod name
          export KAFKA_NODE_ID=${HOSTNAME##*-}

          # Set advertised listeners dynamically
          export KAFKA_ADVERTISED_LISTENERS="PLAINTEXT://${HOSTNAME}.kafka-headless.iot-pipeline.svc.cluster.local:9092"

          echo "===== Starting Kafka Broker ====="
          echo "Node ID: ${KAFKA_NODE_ID}"
          echo "Pod Name: ${HOSTNAME}"
          echo "Advertised Listeners: ${KAFKA_ADVERTISED_LISTENERS}"
          echo "=================================="
          
          # Verify storage is formatted
          if [ ! -f /var/lib/kafka/data/meta.properties ]; then
            echo "ERROR: Storage not formatted! Init container may have failed."
            exit 1
          fi
          
          echo "Storage meta.properties:"
          cat /var/lib/kafka/data/meta.properties
          
          # Start Kafka using Confluent's entrypoint
          exec /etc/confluent/docker/run
        
        env:
        # ===== KRaft Mode Configuration =====
        
        # Process roles: this node is both broker and controller
        - name: KAFKA_PROCESS_ROLES
          value: "broker,controller"
        
        # Node ID: extracted from hostname in startup command
        # Will be 0, 1, or 2 based on pod ordinal
        
        # Controller quorum voters: list of all controller nodes
        # Format: id@host:port,id@host:port,...
        - name: KAFKA_CONTROLLER_QUORUM_VOTERS
          value: "0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093"
        
        # ===== Listener Configuration =====
        
        # Listeners: what ports Kafka listens on
        # PLAINTEXT: client connections (port 9092)
        # CONTROLLER: inter-controller communication (port 9093)
        - name: KAFKA_LISTENERS
          value: "PLAINTEXT://:9092,CONTROLLER://:9093"
        
        # Advertised listeners: set dynamically in startup command
        # (removed from here to avoid variable expansion issues)
        
        # Security protocol map
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
        
        # Inter-broker listener: used for broker-to-broker communication
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "PLAINTEXT"
        
        # Controller listener: used for controller quorum
        - name: KAFKA_CONTROLLER_LISTENER_NAMES
          value: "CONTROLLER"
        
        # ===== Replication & Durability =====
        
        # Default replication factor for auto-created topics
        - name: KAFKA_DEFAULT_REPLICATION_FACTOR
          value: "3"
        
        # Minimum in-sync replicas for producer acks=all
        - name: KAFKA_MIN_INSYNC_REPLICAS
          value: "2"
        
        # Offsets topic replication (internal topic)
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "3"
        
        # Transaction state log settings (for exactly-once semantics)
        - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
          value: "2"
        - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
          value: "3"
        
        # ===== Performance Tuning =====
        
        # Number of threads for network requests
        - name: KAFKA_NUM_NETWORK_THREADS
          value: "8"
        
        # Number of threads for I/O operations
        - name: KAFKA_NUM_IO_THREADS
          value: "8"
        
        # Socket buffer sizes
        - name: KAFKA_SOCKET_SEND_BUFFER_BYTES
          value: "102400"
        - name: KAFKA_SOCKET_RECEIVE_BUFFER_BYTES
          value: "102400"
        - name: KAFKA_SOCKET_REQUEST_MAX_BYTES
          value: "104857600"
        
        # Log flush settings (for durability vs performance)
        - name: KAFKA_LOG_FLUSH_INTERVAL_MESSAGES
          value: "10000"
        - name: KAFKA_LOG_FLUSH_INTERVAL_MS
          value: "1000"
        
        # Consumer group initial rebalance delay
        - name: KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS
          value: "3000"
        
        # ===== Storage Configuration =====
        
        # Where Kafka stores data (must match init container)
        - name: KAFKA_LOG_DIRS
          value: "/var/lib/kafka/data"
        
        # Log retention (7 days)
        - name: KAFKA_LOG_RETENTION_HOURS
          value: "168"
        - name: KAFKA_LOG_RETENTION_BYTES
          value: "1073741824"  # 1GB per partition
        
        # Log segment size
        - name: KAFKA_LOG_SEGMENT_BYTES
          value: "1073741824"  # 1GB
        
        # Compression (reduces storage and network usage)
        - name: KAFKA_COMPRESSION_TYPE
          value: "snappy"
        
        # ===== JMX Monitoring =====
        
        - name: KAFKA_JMX_PORT
          value: "7071"
        - name: KAFKA_JMX_HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: KAFKA_JMX_OPTS
          value: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.rmi.port=7071"
        
        # ===== Java Heap Settings (for local dev) =====
        
        - name: KAFKA_HEAP_OPTS
          value: "-Xmx512M -Xms512M"

        # ===== Other Settings =====
        
        # Auto-create topics when producer sends data
        - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
          value: "true"
        
        # Default number of partitions for auto-created topics
        - name: KAFKA_NUM_PARTITIONS
          value: "6"
        
        # Hostname for pod identity
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        
        # Cluster ID (from secret)
        - name: CLUSTER_ID
          valueFrom:
            secretKeyRef:
              name: iot-pipeline-secrets
              key: CLUSTER_ID
        
        # Volume mounts
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        
        # Resource limits
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        
        # Liveness probe: is Kafka running?
        livenessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 6
        
        # Readiness probe: is Kafka ready for traffic?
        readinessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 6
      
      # Security context (optional, for production)
      # securityContext:
      #   fsGroup: 1000
      #   runAsUser: 1000
      #   runAsNonRoot: true
  
  # Persistent volume claim templates
  volumeClaimTemplates:
  - metadata:
      name: kafka-data
      labels:
        app: kafka
    spec:
      accessModes: 
      - ReadWriteOnce
      storageClassName: standard
      resources:
        requests:
          storage: 5Gi

# # Kafka StatefulSet with KRaft mode (no Zookeeper required)
# # KRaft = Kafka Raft metadata mode, introduced in Kafka 2.8+

# apiVersion: apps/v1
# kind: StatefulSet
# metadata:
#   name: kafka
#   namespace: iot-pipeline
#   labels:
#     app: kafka
#     component: broker
# spec:
#   serviceName: kafka-headless
#   replicas: 3
#   selector:
#     matchLabels:
#       app: kafka
#   # podManagementPolicy: Parallel  # Start all pods simultaneously
#   podManagementPolicy: OrderedReady # Start all pods consecutively
#   template:
#     metadata:
#       labels:
#         app: kafka
#         component: broker
#     spec:
#       # Add a short delay between pod startups
#       terminationGracePeriodSeconds: 120

#       # Init container: Formats Kafka storage for KRaft mode
#       # This runs ONCE before the main Kafka container starts
#       initContainers:
#       - name: kafka-init
#         image: confluentinc/cp-kafka:7.9.0
#         command:
#         - /bin/bash
#         - -c
#         - |
#           set -ex
          
#           # Extract node ID from pod name
#           # kafka-0 -> 0, kafka-1 -> 1, kafka-2 -> 2
#           NODE_ID=${HOSTNAME##*-}

#           echo "===== Kafka Init Container ====="
#           echo "Pod Name: ${HOSTNAME}"
#           echo "Node ID: ${NODE_ID}"
#           echo "Cluster ID: ${CLUSTER_ID}"
#           echo "================================"
          
#           # Check if storage is already formatted
#           # meta.properties file indicates formatted storage
#           if [ -f /var/lib/kafka/data/meta.properties ]; then
#             echo "Storage already formatted, skipping format step"
#             cat /var/lib/kafka/data/meta.properties
#           else
#             echo "Formatting storage for KRaft mode..."
            
#             # Create a temporary server.properties for formatting
#             cat > /tmp/server.properties <<EOF
#           process.roles=broker,controller
#           node.id=${NODE_ID}
#           controller.quorum.voters=0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093
#           listeners=PLAINTEXT://:9092,CONTROLLER://:9093
#           inter.broker.listener.name=PLAINTEXT
#           controller.listener.names=CONTROLLER
#           log.dirs=/var/lib/kafka/data
#           num.partitions=3
#           default.replication.factor=3
#           min.insync.replicas=2
#           offsets.topic.replication.factor=3
#           transaction.state.log.replication.factor=3
#           transaction.state.log.min.isr=2
#           EOF
            
#             # Format the storage with the cluster ID
#             kafka-storage format \
#               --cluster-id="${CLUSTER_ID}" \
#               --config=/tmp/server.properties \
#               --ignore-formatted
            
#             echo "Storage formatting complete"
#             cat /var/lib/kafka/data/meta.properties
#           fi
          
#           echo "Init container completed successfully"
#         env:
#         - name: CLUSTER_ID
#           valueFrom:
#             secretKeyRef:
#               name: iot-pipeline-secrets
#               key: CLUSTER_ID
#         - name: HOSTNAME
#           valueFrom:
#             fieldRef:
#               fieldPath: metadata.name
#         volumeMounts:
#         - name: kafka-data
#           mountPath: /var/lib/kafka/data
#         resources:
#           requests:
#             memory: "256Mi"
#             cpu: "200m"
#           limits:
#             memory: "512Mi"
#             cpu: "500m"

        
#       # Main Kafka container
#       containers:
#       - name: kafka
#         image: confluentinc/cp-kafka:7.9.0
#         ports:
#         - containerPort: 9092
#           name: kafka
#           protocol: TCP
#         - containerPort: 9093
#           name: controller
#           protocol: TCP
#         - containerPort: 7071
#           name: jmx
#           protocol: TCP
        
#         # Startup command
#         command:
#         - /bin/bash
#         - -c
#         - |
#           set -ex

#           # Extract the numeric node ID from the pod name (e.g., kafka-0 -> 0)
#           NODE_ID="${HOSTNAME##*-}"
#           export KAFKA_NODE_ID="$NODE_ID"

#           # Set advertised listeners dynamically using the node ID
#           export KAFKA_ADVERTISED_LISTENERS="PLAINTEXT://kafka-${NODE_ID}.kafka-headless.iot-pipeline.svc.cluster.local:9092"

#           echo "===== Starting Kafka Broker ====="
#           echo "Node ID: ${KAFKA_NODE_ID}"
#           echo "Pod Name: ${HOSTNAME}"
#           echo "Advertised Listeners: ${KAFKA_ADVERTISED_LISTENERS}"
#           echo "=================================="
          
#           # Verify storage is formatted
#           if [ ! -f /var/lib/kafka/data/meta.properties ]; then
#             echo "ERROR: Storage not formatted! Init container may have failed."
#             exit 1
#           fi

#           echo "Storage meta.properties:"
#           cat /var/lib/kafka/data/meta.properties
                  
#           echo "Starting Kafka using KRaft configuraiton..."
#           exec /etc/confluent/docker/run
        
#         env:
#         # ===== KRaft Mode Configuration =====
        
#         # Process roles: this node is both broker and controller
#         - name: KAFKA_PROCESS_ROLES
#           value: "broker,controller"
        
#         # Controller quorum voters: list of all controller nodes
#         # Format: id@host:port,id@host:port,...
#         - name: KAFKA_CONTROLLER_QUORUM_VOTERS
#           value: "0@kafka-0.kafka-headless.iot-pipeline.svc.cluster.local:9093,1@kafka-1.kafka-headless.iot-pipeline.svc.cluster.local:9093,2@kafka-2.kafka-headless.iot-pipeline.svc.cluster.local:9093"
        
#         # ===== Listener Configuration =====
        
#         # Listeners: what ports Kafka listens on
#         # PLAINTEXT: client connections (port 9092)
#         # CONTROLLER: inter-controller communication (port 9093)
#         - name: KAFKA_LISTENERS
#           value: "PLAINTEXT://:9092,CONTROLLER://:9093"
        
#         # Security protocol map
#         - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
#           value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
        
#         # Inter-broker listener: used for broker-to-broker communication
#         - name: KAFKA_INTER_BROKER_LISTENER_NAME
#           value: "PLAINTEXT"
        
#         # Controller listener: used for controller quorum
#         - name: KAFKA_CONTROLLER_LISTENER_NAMES
#           value: "CONTROLLER"
        
#         # ===== Essential Configuraiton =====
        
#         - name: KAFKA_LOG_DIRS
#           value: "/var/lib/kafka/data"

#         # Default replication factor for auto-created topics
#         - name: KAFKA_DEFAULT_REPLICATION_FACTOR
#           value: "3"
        
#         # Minimum in-sync replicas for producer acks=all
#         - name: KAFKA_MIN_INSYNC_REPLICAS
#           value: "2"
        
#         # Offsets topic replication (internal topic)
#         - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
#           value: "3"
        
#         # Transaction state log settings (for exactly-once semantics)
#         - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
#           value: "2"

#         - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
#           value: "3"

#         - name: KAFKA_NUM_PARTITIONS
#           value: "3"
#         - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
#           value: "true"

#         # # ===== Remove deprecated port configuration =====
#         # - name: KAFKA_PORT
#         #   value: ""  # Override any default port setting

#        # ===== Cluster ID and Hostname ===== 

#         # Cluster ID (from secret)
#         - name: CLUSTER_ID
#           valueFrom:
#             secretKeyRef:
#               name: iot-pipeline-secrets
#               key: CLUSTER_ID

#         # Hostname for pod identity
#         - name: HOSTNAME
#           valueFrom:
#             fieldRef:
#               fieldPath: metadata.name

#         # ===== JMX Monitoring =====
        
#         - name: KAFKA_JMX_PORT
#           value: "7071"
#         - name: KAFKA_JMX_HOSTNAME
#           valueFrom:
#             fieldRef:
#               fieldPath: status.podIP
        
#         # ===== Performance =====
        
#         - name: KAFKA_HEAP_OPTS
#           value: "-Xmx512M -Xms512M"

#         - name: KAFKA_NUM_NETWORK_THREADS
#           value: "3"
#         - name: KAFKA_NUM_IO_THREADS
#           value: "8"

#         # Volume mounts
#         volumeMounts:
#         - name: kafka-data
#           mountPath: /var/lib/kafka/data
        
#         # Resource limits
#         resources:
#           requests:
#             memory: "2Gi"
#             cpu: "1000m"
#           limits:
#             memory: "4Gi"
#             cpu: "2000m"
        
#         # Liveness probe: is Kafka running?
#         livenessProbe:
#           tcpSocket:
#             port: 9092
#           initialDelaySeconds: 90
#           periodSeconds: 30
#           timeoutSeconds: 10
#           failureThreshold: 3
        
#         # Readiness probe: is Kafka ready for traffic?
#         readinessProbe:
#           tcpSocket:
#             port: 9092
#           initialDelaySeconds: 60
#           periodSeconds: 10
#           timeoutSeconds: 10
#           failureThreshold: 6

#         # Startup probe for slow-starting applications
#         startupProbe:
#           tcpSocket:
#             port: 9092
#           initialDelaySeconds: 30
#           periodSeconds: 10
#           timeoutSeconds: 5
#           failureThreshold: 30  # Allow up to 5 minutes for startup

#       # Security context (optional, for production)
#       # securityContext:
#       #   fsGroup: 1000
#       #   runAsUser: 1000
#       #   runAsNonRoot: true
  
#   # Persistent volume claim templates
#   volumeClaimTemplates:
#   - metadata:
#       name: kafka-data
#       labels:
#         app: kafka
#     spec:
#       accessModes: 
#       - ReadWriteOnce
#       storageClassName: standard
#       resources:
#         requests:
#           storage: 5Gi